{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": true,
     "title": "Carregando Datasets "
    }
   },
   "outputs": [],
   "source": [
    "#Definições padrões\n",
    "file_type = \"csv\"\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "\n",
    "# Dados do Canadá (CA)\n",
    "file_location_ca = \"/FileStore/tables/CA_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_ca_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_ca)\n",
    "\n",
    "df_ca_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/CA_category_id.json\")\n",
    "\n",
    "\n",
    "# Dados do Brasil (BR)\n",
    "file_location_br = \"/FileStore/tables/BR_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_br_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_br)\n",
    "\n",
    "df_br_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/BR_category_id.json\")\n",
    "\n",
    " # Dados da Alemanha (DE)\n",
    "file_location_de = \"/FileStore/DE_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_de_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_de)\n",
    "\n",
    "df_de_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/DE_category_id.json\")\n",
    "\n",
    "  # Dados da França (FR)\n",
    "file_location_fr = \"/FileStore/FR_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_fr_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_fr)\n",
    "\n",
    "df_fr_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/FR_category_id.json\")\n",
    "\n",
    "\n",
    "  # Dados da Gra-Bretanha (GB)\n",
    "file_location_gb = \"/FileStore/GB_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_gb_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_gb)\n",
    "\n",
    "df_gb_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/GB_category_id.json\")\n",
    "\n",
    "  # Dados da India (IN)\n",
    "file_location_in = \"/FileStore/IN_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_in_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_in)\n",
    "\n",
    "df_in_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/IN_category_id.json\")\n",
    "\n",
    "\n",
    "  # Dados do Japão (JP)\n",
    "file_location_jp = \"/FileStore/JP_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_jp_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_jp)\n",
    "\n",
    "df_jp_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/JP_category_id.json\")\n",
    "\n",
    "\n",
    "  # Dados da Coreia (KR)\n",
    "file_location_kr = \"/FileStore/KR_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_kr_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_kr)\n",
    "\n",
    "df_kr_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/KR_category_id.json\")\n",
    "\n",
    "\n",
    "# Dados do México (MX)\n",
    "file_location_mx  = \"/FileStore/MX_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_mx_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_mx)\n",
    "\n",
    "df_mx_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/MX_category_id.json\")\n",
    "\n",
    "\n",
    "# Dados da Russia (RU)\n",
    "file_location_ru  = \"/FileStore/RU_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_ru_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_ru)\n",
    "\n",
    "df_ru_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/RU_category_id.json\")\n",
    "\n",
    "\n",
    "  # Dados dos EUA (US)\n",
    "file_location_us  = \"/FileStore/US_youtube_trending_data.csv\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_us_csv = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"multiline\",True)\\\n",
    "  .load(file_location_us)\n",
    "\n",
    "df_us_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/US_category_id.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc6fa2d-d85b-4add-a066-f0693ad14ff0",
     "showTitle": true,
     "title": "Manipulando tabelas JSON"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132+---+--------------------+\n",
      "| id|            category|\n",
      "+---+--------------------+\n",
      "|  1|    Film & Animation|\n",
      "| 10|               Music|\n",
      "| 15|      Pets & Animals|\n",
      "| 17|              Sports|\n",
      "| 18|        Short Movies|\n",
      "| 19|     Travel & Events|\n",
      "|  2|    Autos & Vehicles|\n",
      "| 20|              Gaming|\n",
      "| 21|       Videoblogging|\n",
      "| 22|      People & Blogs|\n",
      "| 23|              Comedy|\n",
      "| 24|       Entertainment|\n",
      "| 25|     News & Politics|\n",
      "| 26|       Howto & Style|\n",
      "| 27|           Education|\n",
      "| 28|Science & Technology|\n",
      "| 29|Nonprofits & Acti...|\n",
      "| 30|              Movies|\n",
      "| 31|     Anime/Animation|\n",
      "| 32|    Action/Adventure|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Objetivo: Ter uma tabela com todas as categorias que tem nos arquivos JSON\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col,desc\n",
    "\n",
    "# Unindo todas as tabelas\n",
    "df_json = df_ca_json.union(df_br_json).union(df_de_json).union(df_fr_json).union(df_gb_json).union(df_in_json).union(df_jp_json).union(df_kr_json).union(df_mx_json).union(df_ru_json).union(df_us_json)\n",
    "\n",
    "display(df_json.count())\n",
    "\n",
    "#Transformação dos dados\n",
    "df_json = df_json.select(explode(col(\"items\")).alias(\"element\"))\n",
    "df_json = df_json.select(\"element.id\", \"element.snippet.title\")\n",
    "df_json = df_json.withColumnRenamed(\"title\", \"category\")\n",
    "\n",
    "# Elimando duplicadas\n",
    "df_json = df_json.distinct()\n",
    "\n",
    "\n",
    "\n",
    "display(df_json.count())\n",
    "df_json.orderBy((\"id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf45dd3-602a-43ad-ae1b-d91c3be95738",
     "showTitle": true,
     "title": "Manipulando tabelas CSV"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------+--------------+---------------+----------+--------+-------------+----------+---------+-------------+\n",
      "|                    title|         publishedAt|  channelTitle|       category|categoryId|language|trending_days|     views|    likes|comment_count|\n",
      "+-------------------------+--------------------+--------------+---------------+----------+--------+-------------+----------+---------+-------------+\n",
      "|     Turn into orbeez ...|2021-07-03T04:04:57Z|        FFUNTV|  Entertainment|        24|      US|           36|4992282494|166216047|        66321|\n",
      "|     BLACKPINK - ‘Pink...|2022-08-19T04:00:13Z|     BLACKPINK|          Music|        10|      US|           23|4816767457|266000963|     73459666|\n",
      "|     BLACKPINK - ‘Pink...|2022-08-19T04:00:13Z|     BLACKPINK|          Music|        10|      KR|           20|4052943212|228007915|     63073918|\n",
      "|     $1 vs $1,000,000,...|2023-06-10T16:00:00Z|       MrBeast|  Entertainment|        24|      US|           32|3907827756|140467939|      3788051|\n",
      "|     $1 vs $500,000 Pl...|2023-04-01T20:00:04Z|       MrBeast|  Entertainment|        24|      US|           31|3507390166|132445554|      3949144|\n",
      "|    JISOO - ‘꽃(FLOWER...|2023-03-31T04:00:14Z|     BLACKPINK|          Music|        10|      US|           28|3422500572|217896729|     30477601|\n",
      "|     BLACKPINK - ‘Shut...|2022-09-16T04:00:12Z|     BLACKPINK|          Music|        10|      US|           25|3191937860|188954103|     28275621|\n",
      "|BTS (방탄소년단) 'Butt...|2021-05-21T03:46:13Z|   HYBE LABELS|          Music|        10|      KR|           13|3168484255|197080009|     83315792|\n",
      "|     Watch the uncenso...|2022-03-28T03:06:53Z| Guardian News|News & Politics|        25|      US|           33|3109852918| 45532038|      7941543|\n",
      "|     Rihanna’s FULL Ap...|2023-02-13T03:58:18Z|           NFL|         Sports|        17|      US|           33|2827851076| 78096459|      4493171|\n",
      "|BTS (방탄소년단) 'Butt...|2021-05-21T03:46:13Z|   HYBE LABELS|          Music|        10|      JP|           11|2579687991|164202391|     69465802|\n",
      "|BTS (방탄소년단) 'Perm...|2021-07-09T03:59:12Z|   HYBE LABELS|          Music|        10|      KR|           15|2193542737|169757190|     40327779|\n",
      "|BTS (방탄소년단) 'Dyna...|2020-08-21T03:58:10Z|Big Hit Labels|          Music|        10|      KR|           11|2149517140|155503264|     60408938|\n",
      "|BTS (방탄소년단) 'Dyna...|2020-08-21T03:58:10Z|Big Hit Labels|          Music|        10|      JP|           10|1948824175|140509764|     54785147|\n",
      "|     SHAKIRA || BZRP M...|2023-01-12T00:00:07Z|      Bizarrap|  Entertainment|        24|      MX|           14|1916035568|105538289|      6019719|\n",
      "|BTS (방탄소년단) 'Dyna...|2020-08-21T03:58:10Z|Big Hit Labels|          Music|        10|      IN|           10|1887077945|139247236|     54101978|\n",
      "|     BLACKPINK - ‘Pink...|2022-08-19T04:00:13Z|     BLACKPINK|          Music|        10|      MX|           11|1837080661|114915574|     32238510|\n",
      "|     BLACKPINK - ‘Shut...|2022-09-16T04:00:12Z|     BLACKPINK|          Music|        10|      KR|           16|1814789627|116002269|     17144254|\n",
      "|     BLACKPINK - ‘Pink...|2022-08-19T04:00:13Z|     BLACKPINK|          Music|        10|      BR|           11|1789483583|114103168|     31971489|\n",
      "|BTS (방탄소년단) 'Dyna...|2020-08-21T03:58:10Z|Big Hit Labels|          Music|        10|      MX|            9|1760822595|129361607|     49195579|\n",
      "+-------------------------+--------------------+--------------+---------------+----------+--------+-------------+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Objetivo: Unir todas as tabelas do tipo csv, reduzir o periodo de data, agregações e adições/remoções de coluuna\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, year, when, desc, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Adição de coluna de linguagem para cada tabela\n",
    "df_br_csv = df_br_csv.withColumn(\"language\", lit(\"BR\"))\n",
    "df_ca_csv = df_ca_csv.withColumn(\"language\", lit(\"CA\"))\n",
    "df_de_csv = df_de_csv.withColumn(\"language\", lit(\"DE\"))\n",
    "df_fr_csv = df_fr_csv.withColumn(\"language\", lit(\"FR\"))\n",
    "df_gb_csv = df_gb_csv.withColumn(\"language\", lit(\"GB\"))\n",
    "df_in_csv = df_in_csv.withColumn(\"language\", lit(\"IN\"))\n",
    "df_jp_csv = df_jp_csv.withColumn(\"language\", lit(\"JP\"))\n",
    "df_kr_csv = df_kr_csv.withColumn(\"language\", lit(\"KR\"))\n",
    "df_mx_csv = df_mx_csv.withColumn(\"language\", lit(\"MX\"))\n",
    "df_ru_csv = df_ru_csv.withColumn(\"language\", lit(\"RU\"))\n",
    "df_us_csv = df_us_csv.withColumn(\"language\", lit(\"US\"))\n",
    "\n",
    "#Unindo as tabelas\n",
    "df_csv = df_ca_csv.union(df_br_csv).union(df_de_csv).union(df_fr_csv).union(df_gb_csv).union(df_in_csv).union(df_jp_csv).union(df_kr_csv).union(df_mx_csv).union(df_ru_csv).union(df_us_csv)\n",
    "\n",
    "# Limpando as linhas duplicadas \n",
    "df_csv = df_csv.distinct()\n",
    "\n",
    "# selecionando apenas as colunas que iremos utilizar em pelo menos uma consulta\n",
    "columns_to_keep = [\"title\", \"publishedAt\", \"channelTitle\",\"categoryId\",\"language\",\"view_count\",\"likes\",\"comment_count\"]\n",
    "\n",
    "# adicionar coluna: numero de vezes que se manteve em trend (window function) e cast de valores (String->int)\n",
    "w = Window.partitionBy(\"title\",\"language\")\n",
    "df_csv = df_csv.withColumn(\"trending_days\", count(col(\"title\")).over(w)) \\\n",
    "    .withColumn(\"view_count\", col(\"view_count\").cast(\"int\")) \\\n",
    "    .withColumn(\"likes\", col(\"likes\").cast(\"int\")) \\\n",
    "    .withColumn(\"comment_count\", col(\"comment_count\").cast(\"int\")) \n",
    "\n",
    "# agreção por soma do views, likes, commentarios, deslikes\n",
    "columns_to_keep_agg = [\"title\", \"publishedAt\", \"channelTitle\",\"categoryId\",\"language\",\"trending_days\"]\n",
    "df_csv = df_csv.groupBy(columns_to_keep_agg).agg(sum(col(\"likes\")).alias(\"likes\"),sum(col(\"view_count\")).alias(\"views\")\\\n",
    "                                            ,sum(col(\"comment_count\")).alias(\"comment_count\"))\n",
    "\n",
    "# join com tabela de categorias \n",
    "columns_to_keep = [\"title\", \"publishedAt\", \"channelTitle\",\"category\",\"categoryId\",\"language\",\"trending_days\",\"views\",\"likes\",\"comment_count\"]\n",
    "df_final = df_csv.join(df_json, df_csv.categoryId == df_json.id).drop(df_json.id)\n",
    "\n",
    "df_final.select(columns_to_keep).orderBy(desc(\"views\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf8a8c5-d816-409b-9ae3-7ee94d4a7bc6",
     "showTitle": true,
     "title": "Transformação de dados para Cassandra"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "df_final_c = df_final.drop(\"categoryId\")\n",
    "display(df_final_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09527478-ec0c-40c3-b4be-b8792d3d4d31",
     "showTitle": true,
     "title": "Transformação de dados para neo4j"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, year, when, desc, monotonically_increasing_id,avg,asc\n",
    "\n",
    "# Tabela canal\n",
    "df_canal = df_final.groupBy(\"channelTitle\").agg(sum(col(\"views\")).alias(\"views\"))\n",
    "df_canal = df_canal.withColumn(\"id\", monotonically_increasing_id() + 1)\n",
    "\n",
    "# Tabela grupo\n",
    "#adicionar coluna de faixa de visualizações\n",
    "df_final = df_final.withColumn(\"rangeOfViews\",\n",
    "    when((col(\"views\") >= 0) & (col(\"views\") <= 1000000), \"Grupo 1\") \\\n",
    "    .when((col(\"views\") > 1000000) & (col(\"views\") <= 5000000), \"Grupo 2\") \\\n",
    "    .when(col(\"views\") > 5000000, \"Grupo 3\"))\n",
    "\n",
    "df_grupo = df_final.select(\"rangeOfViews\").distinct()\n",
    "df_grupo = df_grupo.withColumn(\"id\",monotonically_increasing_id() + 1)\n",
    "# Tabela categoria\n",
    "df_categoria = df_final.select(\"categoryId\",\"category\").distinct()\n",
    "df_categoria= df_categoria.withColumnRenamed(\"category\",\"categoryName\")\n",
    "# Tabela paises  \n",
    "df_pais = df_final.select(\"language\").distinct()\n",
    "df_pais = df_pais.withColumn(\"id\", monotonically_increasing_id() + 1)\n",
    "df_pais = df_pais.withColumnRenamed(\"language\",\"languageName\")\n",
    "\n",
    "# Relação canal-categoria\n",
    "df_r_canal_categoria = df_final.select(\"category\",\"channelTitle\").distinct()\n",
    "df_r_canal_categoria = df_r_canal_categoria.where(col(\"channelTitle\").isNotNull() & col(\"category\").isNotNull())\n",
    "\n",
    "# Relaçao canal-paises \n",
    "df_r_canal_paises = df_final.select(\"channelTitle\",\"language\").distinct()\n",
    "df_r_canal_paises = df_r_canal_paises.where(col(\"channelTitle\").isNotNull() & col(\"language\").isNotNull())\n",
    "\n",
    "#Relação entre canal-grupo\n",
    "df_r_canal_grupo = df_final.select(\"channelTitle\",\"rangeOfViews\").distinct()\n",
    "df_r_canal_grupo = df_r_canal_grupo.where(col(\"channelTitle\").isNotNull() & col(\"rangeOfViews\").isNotNull())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4445952-2dad-485e-8bff-602c96271096",
     "showTitle": true,
     "title": "Carregando dados para neo4j"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#instanceid=\"beb1dcaf\"\n",
    "#instancename=\"Instance01\"\n",
    "# Carregando conector\n",
    "\n",
    "#Atributos de credenciais\n",
    "url = \"neo4j+s://beb1dcaf.databases.neo4j.io\"\n",
    "username=\"neo4j\"\n",
    "password=\"Vu41pYnHMzUq30d4Zk6zIeUre1T3K_vwpTKFU4Tq2P8\"\n",
    "#Adição tabela categoria\n",
    "df_categoria.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .mode(\"Overwrite\")\\\n",
    "        .option(\"url\", url)\\\n",
    "        .option(\"authentication.type\", \"basic\")\\\n",
    "        .option(\"authentication.basic.username\", username)\\\n",
    "        .option(\"authentication.basic.password\", password)\\\n",
    "        .option(\"labels\", \":Categoria\")  \\\n",
    "        .option(\"node.keys\", \"categoryId\")\\\n",
    "        .save()\n",
    "#Adição tabela de canal \n",
    "df_canal.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .mode(\"Overwrite\")\\\n",
    "        .option(\"url\", url)\\\n",
    "        .option(\"authentication.type\", \"basic\")\\\n",
    "        .option(\"authentication.basic.username\", username)\\\n",
    "        .option(\"authentication.basic.password\", password)\\\n",
    "        .option(\"labels\", \":Canal\")  \\\n",
    "        .option(\"node.keys\", \"id\")\\\n",
    "        .save()\n",
    "\n",
    "#Adição tabela grupo\n",
    "df_grupo.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .mode(\"Overwrite\")\\\n",
    "        .option(\"url\", url)\\\n",
    "        .option(\"authentication.type\", \"basic\")\\\n",
    "        .option(\"authentication.basic.username\", username)\\\n",
    "        .option(\"authentication.basic.password\", password)\\\n",
    "        .option(\"labels\", \":Grupo\")  \\\n",
    "        .option(\"node.keys\", \"id\")\\\n",
    "        .save()\n",
    "#Adição tabelas paises \n",
    "df_pais.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .mode(\"Overwrite\")\\\n",
    "        .option(\"url\", url)\\\n",
    "        .option(\"authentication.type\", \"basic\")\\\n",
    "        .option(\"authentication.basic.username\", username)\\\n",
    "        .option(\"authentication.basic.password\", password)\\\n",
    "        .option(\"labels\", \":Pais\")  \\\n",
    "        .option(\"node.keys\", \"id\")\\\n",
    "        .save()\n",
    "#Adição relação canal-categoria\n",
    "df_r_canal_categoria.repartition(1).write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", username)\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", \"TEM_VIDEO_NA\")\\\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\\\n",
    "    .option(\"relationship.source.labels\", \":Canal\")\\\n",
    "    .option(\"relationship.source.save.mode\", \"overwrite\")\\\n",
    "    .option(\"relationship.source.node.keys\", \"channelTitle\")\\\n",
    "    .option(\"relationship.target.labels\", \":Categoria\")\\\n",
    "    .option(\"relationship.target.node.keys\", \"category\")\\\n",
    "    .option(\"relationship.target.save.mode\", \"overwrite\")\\\n",
    "    .save()\n",
    "#Adição relação canal-paises\n",
    "df_r_canal_paises.repartition(1).write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", username)\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", \"TEM_VIDEO_DE\")\\\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\\\n",
    "    .option(\"relationship.source.labels\", \":Canal\")\\\n",
    "    .option(\"relationship.source.save.mode\", \"overwrite\")\\\n",
    "    .option(\"relationship.source.node.keys\", \"channelTitle\")\\\n",
    "    .option(\"relationship.target.labels\", \":Pais\")\\\n",
    "    .option(\"relationship.target.node.keys\", \"language\")\\\n",
    "    .option(\"relationship.target.save.mode\", \"overwrite\")\\\n",
    "    .save()\n",
    "#Adição relação canal-grupo\n",
    "df_r_canal_grupo.repartition(1).write.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", username)\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"relationship\", \"TEM_VIDEO_NO\")\\\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\\\n",
    "    .option(\"relationship.source.labels\", \":Canal\")\\\n",
    "    .option(\"relationship.source.save.mode\", \"overwrite\")\\\n",
    "    .option(\"relationship.source.node.keys\", \"channelTitle\")\\\n",
    "    .option(\"relationship.target.labels\", \":Grupo\")\\\n",
    "    .option(\"relationship.target.node.keys\", \"rangeOfViews\")\\\n",
    "    .option(\"relationship.target.save.mode\", \"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a1ff3d-01da-42d7-b0ca-44aade1a23e2",
     "showTitle": true,
     "title": "Lendo dados do neo4j"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "url = \"neo4j+s://beb1dcaf.databases.neo4j.io\"\n",
    "username=\"neo4j\"\n",
    "password=\"Vu41pYnHMzUq30d4Zk6zIeUre1T3K_vwpTKFU4Tq2P8\"\n",
    "\n",
    "test_read = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"authentication.type\", \"basic\")\\\n",
    "    .option(\"authentication.basic.username\", username)\\\n",
    "    .option(\"authentication.basic.password\", password)\\\n",
    "    .option(\"labels\", \":Categoria\")\\\n",
    "    .load()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions._\n",
    "import com.datastax.spark.connector._\n",
    "import org.apache.spark.sql.cassandra._\n",
    "import spark.implicits._\n",
    "\n",
    "val dbName = spark.sparkContext.getConf.get(\"spark.database.name\")\n",
    "val keyspace = spark.sparkContext.getConf.get(\"spark.keyspace.name\")\n",
    "\n",
    "spark.conf.set(s\"spark.sql.catalog.$dbName\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")\n",
    "spark.sql(s\"use $dbName.$keyspace\")\n",
    "\n",
    "val video_trends_table = spark.sql(\"select * from df_final_c\")\n",
    "\n",
    "video_trends_table.createCassandraTable(keyspace, \"video_trends\")\n",
    "\n",
    "video_trends_table.write.cassandraFormat(\"video_trends\", keyspace).mode(\"append\").save()\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "val data = sc.cassandraTable(keyspace, \"video_trends\")\n",
    "\n",
    "data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions._\n",
    "import com.datastax.spark.connector._\n",
    "import org.apache.spark.sql.cassandra._\n",
    "import spark.implicits._\n",
    "import com.datastax.spark.connector.cql.ClusteringColumn\n",
    "\n",
    "val dbName = spark.sparkContext.getConf.get(\"spark.database.name\")\n",
    "val keyspace = spark.sparkContext.getConf.get(\"spark.keyspace.name\")\n",
    "\n",
    "spark.conf.set(s\"spark.sql.catalog.$dbName\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")\n",
    "spark.sql(s\"use $dbName.$keyspace\")\n",
    "\n",
    "val video_trends_table = spark.sql(\"select * from df_final_c\")\n",
    "\n",
    "video_trends_table.createCassandraTableEx(keyspace, \"video_trends_language_category\", Seq(\"language\", \"category\"), Seq((\"views\", ClusteringColumn.Descending), (\"trending_days\", ClusteringColumn.Descending)))\n",
    "\n",
    "video_trends_table.write.cassandraFormat(\"video_trends_language_category\", keyspace).mode(\"append\").save()\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "val data = sc.cassandraTable(keyspace, \"video_trends_language_category\")\n",
    "\n",
    "data.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2547240325085761,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PMD - Projeto Prático",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
